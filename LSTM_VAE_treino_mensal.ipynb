{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxo2YX5Ngnnh6ZOC1e6jXY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luiz-oliveir/lstm_mensal/blob/main/LSTM_VAE_treino_mensal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTm4CHOnD2uL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
        "import pickle\n",
        "import os\n",
        "from datetime import datetime\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                   format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "class ReparameterizationLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(ReparameterizationLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch_size = tf.shape(z_mean)[0]\n",
        "        latent_dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.random.normal(shape=(batch_size, latent_dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "class RepeatVectorLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, timesteps, **kwargs):\n",
        "        super(RepeatVectorLayer, self).__init__(**kwargs)\n",
        "        self.timesteps = timesteps\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.repeat(tf.expand_dims(inputs, axis=1), repeats=self.timesteps, axis=1)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(RepeatVectorLayer, self).get_config()\n",
        "        config.update({'timesteps': self.timesteps})\n",
        "        return config\n",
        "\n",
        "class TemperatureWeightLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(TemperatureWeightLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, x):\n",
        "        mean = tf.keras.backend.mean(x, axis=[1, 2], keepdims=True)\n",
        "        std = tf.keras.backend.std(x, axis=[1, 2], keepdims=True) + tf.keras.backend.epsilon()\n",
        "        z_scores = tf.abs((x - mean) / std)\n",
        "        weights = tf.exp(z_scores)\n",
        "        weights = weights / (tf.keras.backend.mean(weights, axis=[1, 2], keepdims=True) + tf.keras.backend.epsilon())\n",
        "        return weights\n",
        "\n",
        "class LSTM_VAE(tf.keras.Model):\n",
        "    def __init__(self, timesteps=7, n_features=1, latent_dim=32):\n",
        "        super(LSTM_VAE, self).__init__()\n",
        "        self.timesteps = timesteps\n",
        "        self.n_features = n_features\n",
        "        self.latent_dim = latent_dim\n",
        "        self.encoder_lstm_units = 64\n",
        "        self.decoder_lstm_units = 64\n",
        "\n",
        "        # Encoder layers\n",
        "        self.encoder_lstm = tf.keras.layers.LSTM(\n",
        "            self.encoder_lstm_units,\n",
        "            return_sequences=True,\n",
        "            name='encoder_lstm'\n",
        "        )\n",
        "\n",
        "        self.encoder_mean = tf.keras.layers.Dense(\n",
        "            self.latent_dim,\n",
        "            name='encoder_mean'\n",
        "        )\n",
        "\n",
        "        self.encoder_log_var = tf.keras.layers.Dense(\n",
        "            self.latent_dim,\n",
        "            name='encoder_log_var'\n",
        "        )\n",
        "\n",
        "        # Reparameterization layer\n",
        "        self.reparameterize_layer = ReparameterizationLayer(name='reparameterize')\n",
        "\n",
        "        # Decoder layers\n",
        "        self.repeat_vector = RepeatVectorLayer(\n",
        "            timesteps=self.timesteps,\n",
        "            name='repeat_vector'\n",
        "        )\n",
        "\n",
        "        self.decoder_lstm = tf.keras.layers.LSTM(\n",
        "            self.decoder_lstm_units,\n",
        "            return_sequences=True,\n",
        "            name='decoder_lstm'\n",
        "        )\n",
        "\n",
        "        self.decoder_dense = tf.keras.layers.Dense(\n",
        "            self.n_features,\n",
        "            name='decoder_dense'\n",
        "        )\n",
        "\n",
        "        # Temperature weight layer\n",
        "        self.temp_weight_layer = TemperatureWeightLayer(name='temp_weight')\n",
        "\n",
        "        # Loss tracking\n",
        "        self.total_loss_tracker = tf.keras.metrics.Mean(name='total_loss')\n",
        "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(name='reconstruction_loss')\n",
        "        self.kl_loss_tracker = tf.keras.metrics.Mean(name='kl_loss')\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker\n",
        "        ]\n",
        "\n",
        "    def encode(self, x):\n",
        "        encoder_output = self.encoder_lstm(x)\n",
        "        z_mean = self.encoder_mean(encoder_output[:, -1, :])\n",
        "        z_log_var = self.encoder_log_var(encoder_output[:, -1, :])\n",
        "        return z_mean, z_log_var\n",
        "\n",
        "    def decode(self, z):\n",
        "        x = self.repeat_vector(z)\n",
        "        x = self.decoder_lstm(x)\n",
        "        return self.decoder_dense(x)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        z_mean, z_log_var = self.encode(inputs)\n",
        "        z = self.reparameterize_layer([z_mean, z_log_var])\n",
        "        reconstructed = self.decode(z)\n",
        "\n",
        "        if training:\n",
        "            # Reconstruction loss with temperature weights\n",
        "            temp_weights = self.temp_weight_layer(inputs)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                temp_weights * tf.square(inputs - reconstructed)\n",
        "            )\n",
        "\n",
        "            # KL loss\n",
        "            kl_loss = -0.5 * tf.reduce_mean(\n",
        "                1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
        "            )\n",
        "\n",
        "            # Total loss\n",
        "            total_loss = reconstruction_loss + 0.1 * kl_loss\n",
        "\n",
        "            # Update metrics\n",
        "            self.total_loss_tracker.update_state(total_loss)\n",
        "            self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "            self.kl_loss_tracker.update_state(kl_loss)\n",
        "\n",
        "            self.add_loss(total_loss)\n",
        "\n",
        "        return reconstructed\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            reconstructed = self(data, training=True)\n",
        "\n",
        "        # Get trainable variables\n",
        "        trainable_vars = self.trainable_variables\n",
        "\n",
        "        # Calculate gradients\n",
        "        gradients = tape.gradient(self.losses, trainable_vars)\n",
        "\n",
        "        # Apply gradients\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        return {\n",
        "            'loss': self.total_loss_tracker.result(),\n",
        "            'reconstruction_loss': self.reconstruction_loss_tracker.result(),\n",
        "            'kl_loss': self.kl_loss_tracker.result()\n",
        "        }\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(LSTM_VAE, self).get_config()\n",
        "        config.update({\n",
        "            'timesteps': self.timesteps,\n",
        "            'n_features': self.n_features,\n",
        "            'latent_dim': self.latent_dim,\n",
        "            'encoder_lstm_units': self.encoder_lstm_units,\n",
        "            'decoder_lstm_units': self.decoder_lstm_units\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "def prepare_sequences(data, seq_length):\n",
        "    \"\"\"Prepare sequences for LSTM VAE with improved preprocessing\"\"\"\n",
        "    sequences = []\n",
        "    for i in range(len(data) - seq_length + 1):\n",
        "        sequences.append(data[i:i + seq_length])\n",
        "    return np.array(sequences)\n",
        "\n",
        "def train_monthly_model(sequences, timesteps=7, n_features=1, batch_size=64,\n",
        "                       latent_dim=32, epochs=100):\n",
        "    \"\"\"Train LSTM VAE model for a specific month\"\"\"\n",
        "    try:\n",
        "        # Create and compile model\n",
        "        model = LSTM_VAE(\n",
        "            timesteps=timesteps,\n",
        "            n_features=n_features,\n",
        "            latent_dim=latent_dim\n",
        "        )\n",
        "\n",
        "        # Configure optimizer with gradient clipping\n",
        "        optimizer = tf.keras.optimizers.Adam(\n",
        "            learning_rate=0.001,\n",
        "            clipnorm=1.0\n",
        "        )\n",
        "\n",
        "        # Compile model\n",
        "        model.compile(optimizer=optimizer)\n",
        "\n",
        "        # Configure callbacks\n",
        "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='loss',\n",
        "            patience=10,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "\n",
        "        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='loss',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            min_lr=0.00001\n",
        "        )\n",
        "\n",
        "        # Train model\n",
        "        history = model.fit(\n",
        "            sequences,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=[early_stopping, reduce_lr],\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in train_monthly_model: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def train_monthly_models(data_path, output_dir, seq_length=7, batch_size=64,\n",
        "                        latent_dim=32, epochs=100):\n",
        "    \"\"\"Train separate LSTM VAE models for each month with improved extreme handling\"\"\"\n",
        "    try:\n",
        "        # Create output directory\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Define month name mapping\n",
        "        month_map = {\n",
        "            'jan': 'jan', 'feb': 'fev', 'mar': 'mar', 'abr': 'abr',\n",
        "            'may': 'mai', 'jun': 'jun', 'jul': 'jul', 'ago': 'ago',\n",
        "            'sep': 'set', 'oct': 'out', 'nov': 'nov', 'dec': 'dez'\n",
        "        }\n",
        "\n",
        "        # Load and preprocess data\n",
        "        df = pd.read_excel(data_path)\n",
        "        df['Data'] = pd.to_datetime(df['Data'], format='%d/%m/%Y')  # Brazilian date format\n",
        "        df['month'] = df['Data'].dt.strftime('%b').str.lower()\n",
        "        df['month'] = df['month'].map(month_map)  # Convert to Portuguese month names\n",
        "\n",
        "        # Process each month\n",
        "        for month_en, month_pt in month_map.items():\n",
        "            logging.info(f\"Training model for month: {month_pt}\")\n",
        "\n",
        "            try:\n",
        "                # Filter data for current month\n",
        "                month_data = df[df['month'] == month_pt]['Temperatura Maxima'].values\n",
        "\n",
        "                # Check if we have enough data\n",
        "                if len(month_data) < seq_length + 1:\n",
        "                    logging.warning(f\"Insufficient data for month {month_pt}. Skipping...\")\n",
        "                    continue\n",
        "\n",
        "                # Reshape data for scaling\n",
        "                month_data = month_data.reshape(-1, 1)\n",
        "\n",
        "                # Scale data\n",
        "                scaler = RobustScaler()\n",
        "                scaled_data = scaler.fit_transform(month_data)\n",
        "\n",
        "                # Prepare sequences\n",
        "                sequences = prepare_sequences(scaled_data, seq_length)\n",
        "\n",
        "                # Check if we have enough sequences\n",
        "                if len(sequences) < batch_size:\n",
        "                    logging.warning(f\"Not enough sequences for month {month_pt}. Minimum required: {batch_size}. Got: {len(sequences)}. Skipping...\")\n",
        "                    continue\n",
        "\n",
        "                # Convert to numpy array\n",
        "                sequences = np.array(sequences)\n",
        "\n",
        "                # Train model\n",
        "                model = train_monthly_model(\n",
        "                    sequences=sequences,\n",
        "                    timesteps=seq_length,\n",
        "                    n_features=1,\n",
        "                    batch_size=batch_size,\n",
        "                    latent_dim=latent_dim,\n",
        "                    epochs=epochs\n",
        "                )\n",
        "\n",
        "                if model is not None:\n",
        "                    # Create model directory if it doesn't exist\n",
        "                    os.makedirs(os.path.join(output_dir), exist_ok=True)\n",
        "\n",
        "                    # Save model and scaler\n",
        "                    model_path = os.path.join(output_dir, f'lstm_vae_model_{month_pt}.keras')\n",
        "                    scaler_path = os.path.join(output_dir, f'scaler_{month_pt}.pkl')\n",
        "\n",
        "                    model.save(model_path)\n",
        "                    with open(scaler_path, 'wb') as f:\n",
        "                        pickle.dump(scaler, f)\n",
        "\n",
        "                    logging.info(f\"Successfully trained and saved model for {month_pt}\")\n",
        "                else:\n",
        "                    logging.warning(f\"Model training failed for month {month_pt}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error training model for {month_pt}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        logging.info(\"Finished training all monthly models\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error training models: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Configure logging\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
        "    )\n",
        "\n",
        "    # Set paths\n",
        "    data_path = \"../Convencionais processadas temperaturas/82024.xlsx\"\n",
        "    output_dir = \"./lstm_vae_model\"\n",
        "\n",
        "    # Train models\n",
        "    train_monthly_models(\n",
        "        data_path=data_path,\n",
        "        output_dir=output_dir,\n",
        "        seq_length=7,\n",
        "        batch_size=64,\n",
        "        latent_dim=32,\n",
        "        epochs=100\n",
        "    )\n"
      ]
    }
  ]
}