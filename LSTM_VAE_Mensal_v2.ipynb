{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQMiXEqtWFbLf8Kj/aQn/a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luiz-oliveir/lstm_mensal/blob/main/LSTM_VAE_Mensal_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/luiz-oliveir/LSTM_mensal.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jrk0Xsz0DZF-",
        "outputId": "cddada56-b873-447a-c480-8d93a7548399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LSTM_mensal'...\n",
            "remote: Enumerating objects: 86, done.\u001b[K\n",
            "remote: Counting objects: 100% (86/86), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 86 (delta 11), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (86/86), 14.20 MiB | 15.58 MiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import datetime\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import pickle\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('lstm_vae_mensal.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Configurações\n",
        "row_mark = 740\n",
        "batch_size = 128\n",
        "timesteps = 7  # Janela de tempo para análise\n",
        "n_features = 1  # Número de features (temperatura)\n",
        "latent_dim = 32  # Dimensão do espaço latente\n",
        "epoch_num = 100\n",
        "threshold = None\n",
        "\n",
        "# Diretórios\n",
        "base_dir = os.path.abspath(os.path.dirname(''))\n",
        "data_dir = r\"C:\\Users\\Augusto-PC\\Documents\\GitHub\\LSTM\\Convencionais processadas temperaturas\"  # Correct path\n",
        "model_dir = os.path.join(base_dir, \"lstm_vae_model\")\n",
        "images_dir = os.path.join(base_dir, \"lstm_vae_images\")\n",
        "results_dir = os.path.join(base_dir, \"Resumo resultados\")\n",
        "\n",
        "# Verificar e criar diretórios\n",
        "for dir_path in [model_dir, images_dir, results_dir]:\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "        logging.info(f\"Directory created: {dir_path}\")\n",
        "    else:\n",
        "        logging.info(f\"Directory verified: {dir_path}\")\n",
        "\n",
        "# Dicionário de meses\n",
        "meses = {\n",
        "    1:'jan', 2:'fev', 3:'mar', 4:'abr', 5:'mai', 6:'jun',\n",
        "    7:'jul', 8:'ago', 9:'set', 10:'out', 11:'nov', 12:'dez'\n",
        "}\n",
        "\n",
        "class ReparameterizationLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(ReparameterizationLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch_size = tf.shape(z_mean)[0]\n",
        "        latent_dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.random.normal(shape=(batch_size, latent_dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "class RepeatVectorLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, timesteps, **kwargs):\n",
        "        super(RepeatVectorLayer, self).__init__(**kwargs)\n",
        "        self.timesteps = timesteps\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.repeat(tf.expand_dims(inputs, axis=1), repeats=self.timesteps, axis=1)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(RepeatVectorLayer, self).get_config()\n",
        "        config.update({'timesteps': self.timesteps})\n",
        "        return config\n",
        "\n",
        "class TemperatureWeightLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(TemperatureWeightLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, x):\n",
        "        mean = tf.keras.backend.mean(x, axis=[1, 2], keepdims=True)\n",
        "        std = tf.keras.backend.std(x, axis=[1, 2], keepdims=True) + tf.keras.backend.epsilon()\n",
        "        z_scores = tf.abs((x - mean) / std)\n",
        "        weights = tf.exp(z_scores)\n",
        "        weights = weights / (tf.keras.backend.mean(weights, axis=[1, 2], keepdims=True) + tf.keras.backend.epsilon())\n",
        "        return weights\n",
        "\n",
        "class LSTM_VAE(tf.keras.Model):\n",
        "    def __init__(self, timesteps=7, n_features=1, latent_dim=32, **kwargs):\n",
        "        super(LSTM_VAE, self).__init__(**kwargs)\n",
        "        self.timesteps = timesteps\n",
        "        self.n_features = n_features\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # First LSTM layer\n",
        "        self.lstm = tf.keras.layers.LSTM(32, return_sequences=True, name='lstm')\n",
        "\n",
        "        # First LSTM output processing\n",
        "        self.dense = tf.keras.layers.Dense(4, name='dense')\n",
        "        self.dense_1 = tf.keras.layers.Dense(4, name='dense_1')\n",
        "        self.dense_2 = tf.keras.layers.Dense(32, name='dense_2')\n",
        "\n",
        "        # Second LSTM layer\n",
        "        self.lstm_1 = tf.keras.layers.LSTM(32, return_sequences=True, name='lstm_1')\n",
        "\n",
        "        # Second LSTM output processing\n",
        "        self.dense_3 = tf.keras.layers.Dense(32, name='dense_3')\n",
        "        self.dense_4 = tf.keras.layers.Dense(16, name='dense_4')\n",
        "        self.dense_5 = tf.keras.layers.Dense(1, name='dense_5')\n",
        "\n",
        "        # Final processing branch\n",
        "        self.dense_6 = tf.keras.layers.Dense(16, name='dense_6')\n",
        "        self.dense_7 = tf.keras.layers.Dense(1, name='dense_7')\n",
        "\n",
        "        # Additional layers\n",
        "        self.repeat_vector = tf.keras.layers.RepeatVector(timesteps, name='repeat_vector')\n",
        "        self.dropout = tf.keras.layers.Dropout(0.2, name='dropout')\n",
        "        self.layer_norm = tf.keras.layers.LayerNormalization(name='layer_normalization')\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        # First LSTM processing\n",
        "        x = self.lstm(inputs)\n",
        "\n",
        "        # Process first LSTM output\n",
        "        x = self.dense(x)\n",
        "        x = self.dense_1(x)\n",
        "        x = self.dense_2(x)\n",
        "\n",
        "        # Second LSTM processing\n",
        "        x = self.lstm_1(x)\n",
        "\n",
        "        # Get last timestep for repeat vector\n",
        "        last_timestep = x[:, -1, :]\n",
        "        x = self.repeat_vector(last_timestep)\n",
        "\n",
        "        # Process repeated vector\n",
        "        x = self.dense_3(x)\n",
        "        x = self.dense_4(x)\n",
        "\n",
        "        # Apply dropout during training\n",
        "        if training:\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        # Apply layer normalization\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        # Generate outputs through two branches\n",
        "        output1 = self.dense_5(x)\n",
        "        x = self.dense_6(x)\n",
        "        output2 = self.dense_7(x)\n",
        "\n",
        "        # Combine outputs\n",
        "        outputs = tf.concat([output1, output2], axis=-1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(LSTM_VAE, self).get_config()\n",
        "        config.update({\n",
        "            'timesteps': self.timesteps,\n",
        "            'n_features': self.n_features,\n",
        "            'latent_dim': self.latent_dim\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "def reshape(da):\n",
        "    \"\"\"Reshape dados para formato LSTM\"\"\"\n",
        "    data = []\n",
        "    for i in range(len(da) - timesteps + 1):\n",
        "        data.append(da[i:(i + timesteps)])\n",
        "    return np.array(data)\n",
        "\n",
        "def prepare_training_data(data, batch_size=128):\n",
        "    \"\"\"Prepara dados para o modelo\"\"\"\n",
        "    data = reshape(data)\n",
        "    data = data.reshape(-1, timesteps, n_features)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
        "    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "def calculate_advanced_metrics(predictions, originals):\n",
        "    \"\"\"Calculate advanced evaluation metrics\"\"\"\n",
        "    mse = np.mean(np.square(predictions - originals))\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = np.mean(np.abs(predictions - originals))\n",
        "    mape = np.mean(np.abs((originals - predictions) / originals)) * 100\n",
        "\n",
        "    # Calculate log likelihood metrics\n",
        "    residuals = predictions - originals\n",
        "    std = np.std(residuals)\n",
        "    log_likelihood = -0.5 * np.log(2 * np.pi * std**2) - 0.5 * (residuals**2) / (std**2)\n",
        "    mean_log_likelihood = np.mean(log_likelihood)\n",
        "\n",
        "    # Calculate percentile analysis\n",
        "    percentiles = [1, 5, 25, 50, 75, 95, 99]\n",
        "    orig_percentiles = np.percentile(originals, percentiles)\n",
        "    pred_percentiles = np.percentile(predictions, percentiles)\n",
        "\n",
        "    percentile_errors = {\n",
        "        f'p{p}_error': abs(o - p)\n",
        "        for p, o, p in zip(percentiles, orig_percentiles, pred_percentiles)\n",
        "    }\n",
        "\n",
        "    metrics = {\n",
        "        'mse': mse,\n",
        "        'rmse': rmse,\n",
        "        'mae': mae,\n",
        "        'mape': mape,\n",
        "        'mean_log_likelihood': mean_log_likelihood,\n",
        "        **percentile_errors\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def train_monthly_models(excel_files):\n",
        "    \"\"\"Train separate LSTM VAE models for each month\"\"\"\n",
        "    for file_path in excel_files:\n",
        "        logging.info(f\"Processing file: {file_path}\")\n",
        "\n",
        "        try:\n",
        "            # Read data\n",
        "            df = pd.read_excel(file_path)\n",
        "            df['Data'] = pd.to_datetime(df['Data'])\n",
        "            df.set_index('Data', inplace=True)\n",
        "\n",
        "            # Sort index to ensure chronological order\n",
        "            df = df.sort_index()\n",
        "\n",
        "            # Process each month\n",
        "            for month in range(1, 13):\n",
        "                month_data = df[df.index.month == month]\n",
        "\n",
        "                if len(month_data) < batch_size:\n",
        "                    logging.warning(f\"Insufficient data for month {meses[month]} in {os.path.basename(file_path)}\")\n",
        "                    continue\n",
        "\n",
        "                logging.info(f\"Training model for month: {meses[month]} from file {os.path.basename(file_path)}\")\n",
        "\n",
        "                # Scale data\n",
        "                scaler = MinMaxScaler()\n",
        "                scaled_data = scaler.fit_transform(month_data.values.reshape(-1, 1))\n",
        "\n",
        "                # Save scaler\n",
        "                scaler_path = os.path.join(model_dir, f'scaler_{meses[month]}.pkl')\n",
        "                with open(scaler_path, 'wb') as f:\n",
        "                    pickle.dump(scaler, f)\n",
        "\n",
        "                # Prepare training data\n",
        "                train_dataset = prepare_training_data(scaled_data, batch_size)\n",
        "\n",
        "                # Create and compile model\n",
        "                model = LSTM_VAE(timesteps=timesteps, n_features=n_features, latent_dim=latent_dim)\n",
        "\n",
        "                # Build model with input shape\n",
        "                dummy_input = tf.zeros((1, timesteps, n_features))\n",
        "                _ = model(dummy_input, training=False)\n",
        "\n",
        "                # Compile model\n",
        "                model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "                # Add callbacks\n",
        "                callbacks = [\n",
        "                    tf.keras.callbacks.EarlyStopping(\n",
        "                        monitor='val_loss',\n",
        "                        patience=10,\n",
        "                        restore_best_weights=True\n",
        "                    ),\n",
        "                    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "                        monitor='val_loss',\n",
        "                        factor=0.5,\n",
        "                        patience=5,\n",
        "                        min_lr=1e-6\n",
        "                    )\n",
        "                ]\n",
        "\n",
        "                # Train model\n",
        "                history = model.fit(\n",
        "                    train_dataset,\n",
        "                    epochs=epoch_num,\n",
        "                    validation_split=0.2,\n",
        "                    callbacks=callbacks,\n",
        "                    verbose=1\n",
        "                )\n",
        "\n",
        "                # Save model\n",
        "                model_path = os.path.join(model_dir, f'lstm_vae_model_{meses[month]}.h5')\n",
        "                model.save(model_path)\n",
        "\n",
        "                # Plot and save training history\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                plt.plot(history.history['loss'], label='Training Loss')\n",
        "                plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "                plt.title(f'Training History - Month: {meses[month]}')\n",
        "                plt.xlabel('Epoch')\n",
        "                plt.ylabel('Loss')\n",
        "                plt.legend()\n",
        "                plt.savefig(os.path.join(images_dir, f'training_history_{meses[month]}.png'))\n",
        "                plt.close()\n",
        "\n",
        "                logging.info(f\"Model for month {meses[month]} saved successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error processing file {file_path}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "def main():\n",
        "    # Find all Excel files in the data directory\n",
        "    excel_files = glob.glob(os.path.join(data_dir, \"*.xlsx\"))\n",
        "\n",
        "    if not excel_files:\n",
        "        logging.error(f\"No Excel files found in {data_dir}\")\n",
        "        return\n",
        "\n",
        "    logging.info(f\"Found {len(excel_files)} Excel files in {data_dir}\")\n",
        "\n",
        "    try:\n",
        "        train_monthly_models(excel_files)\n",
        "        logging.info(\"Training completed successfully\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during training: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "oXd3krxYDTAN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "417b50e9-60b3-4750-fed9-07ae0ff54513"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:No Excel files found in /Convencionais processadas temperaturas - Copia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0xye0llDMuo"
      },
      "outputs": [],
      "source": []
    }
  ]
}