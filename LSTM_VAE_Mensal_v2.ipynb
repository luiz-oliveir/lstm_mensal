{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsKZGNANHnk5si5mcYCwT4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luiz-oliveir/lstm_mensal/blob/main/LSTM_VAE_Mensal_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/luiz-oliveir/LSTM_mensal.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jrk0Xsz0DZF-",
        "outputId": "cddada56-b873-447a-c480-8d93a7548399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LSTM_mensal'...\n",
            "remote: Enumerating objects: 86, done.\u001b[K\n",
            "remote: Counting objects: 100% (86/86), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 86 (delta 11), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (86/86), 14.20 MiB | 15.58 MiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import datetime\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import pickle\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('lstm_vae_mensal.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Configurações\n",
        "row_mark = 740\n",
        "batch_size = 128\n",
        "timesteps = 7  # Janela de tempo para análise\n",
        "n_features = 1  # Número de features (temperatura)\n",
        "latent_dim = 32  # Dimensão do espaço latente\n",
        "epoch_num = 100\n",
        "threshold = None\n",
        "\n",
        "# Diretórios\n",
        "base_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "data_dir = os.path.join(os.path.dirname(base_dir), \"Convencionais processadas temperaturas\")\n",
        "model_dir = os.path.join(base_dir, \"lstm_vae_model\")\n",
        "images_dir = os.path.join(base_dir, \"lstm_vae_images\")\n",
        "results_dir = os.path.join(base_dir, \"Resumo resultados\")\n",
        "\n",
        "# Verificar e criar diretórios\n",
        "for dir_path in [model_dir, images_dir, results_dir]:\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "        logging.info(f\"Directory created: {dir_path}\")\n",
        "    else:\n",
        "        logging.info(f\"Directory verified: {dir_path}\")\n",
        "\n",
        "# Dicionário de meses\n",
        "meses = {\n",
        "    1:'jan', 2:'fev', 3:'mar', 4:'abr', 5:'mai', 6:'jun',\n",
        "    7:'jul', 8:'ago', 9:'set', 10:'out', 11:'nov', 12:'dez'\n",
        "}\n",
        "\n",
        "class ReparameterizationLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(ReparameterizationLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch_size = tf.shape(z_mean)[0]\n",
        "        latent_dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.random.normal(shape=(batch_size, latent_dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "class RepeatVectorLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, timesteps, **kwargs):\n",
        "        super(RepeatVectorLayer, self).__init__(**kwargs)\n",
        "        self.timesteps = timesteps\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.repeat(tf.expand_dims(inputs, axis=1), repeats=self.timesteps, axis=1)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(RepeatVectorLayer, self).get_config()\n",
        "        config.update({'timesteps': self.timesteps})\n",
        "        return config\n",
        "\n",
        "class TemperatureWeightLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(TemperatureWeightLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, x):\n",
        "        mean = tf.keras.backend.mean(x, axis=[1, 2], keepdims=True)\n",
        "        std = tf.keras.backend.std(x, axis=[1, 2], keepdims=True) + tf.keras.backend.epsilon()\n",
        "        z_scores = tf.abs((x - mean) / std)\n",
        "        weights = tf.exp(z_scores)\n",
        "        weights = weights / (tf.keras.backend.mean(weights, axis=[1, 2], keepdims=True) + tf.keras.backend.epsilon())\n",
        "        return weights\n",
        "\n",
        "class LSTM_VAE(tf.keras.Model):\n",
        "    def __init__(self, timesteps=7, n_features=1, latent_dim=32, **kwargs):\n",
        "        super(LSTM_VAE, self).__init__(**kwargs)\n",
        "        self.timesteps = timesteps\n",
        "        self.n_features = n_features\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # First LSTM layer\n",
        "        self.lstm = tf.keras.layers.LSTM(32, return_sequences=True, name='lstm')  # Output shape: (batch_size, timesteps, 32)\n",
        "\n",
        "        # First LSTM output processing\n",
        "        self.dense = tf.keras.layers.Dense(4, name='dense')  # Output shape: (batch_size, timesteps, 4)\n",
        "        self.dense_1 = tf.keras.layers.Dense(4, name='dense_1')  # Output shape: (batch_size, timesteps, 4)\n",
        "        self.dense_2 = tf.keras.layers.Dense(32, name='dense_2')  # Output shape: (batch_size, timesteps, 32)\n",
        "\n",
        "        # Second LSTM layer\n",
        "        self.lstm_1 = tf.keras.layers.LSTM(32, return_sequences=True, name='lstm_1')  # Output shape: (batch_size, timesteps, 32)\n",
        "\n",
        "        # Second LSTM output processing\n",
        "        self.dense_3 = tf.keras.layers.Dense(32, name='dense_3')  # Output shape: (batch_size, timesteps, 32)\n",
        "        self.dense_4 = tf.keras.layers.Dense(16, name='dense_4')  # Output shape: (batch_size, timesteps, 16)\n",
        "        self.dense_5 = tf.keras.layers.Dense(1, name='dense_5')  # Output shape: (batch_size, timesteps, 1)\n",
        "\n",
        "        # Final processing branch\n",
        "        self.dense_6 = tf.keras.layers.Dense(16, name='dense_6')  # Output shape: (batch_size, timesteps, 16)\n",
        "        self.dense_7 = tf.keras.layers.Dense(1, name='dense_7')  # Output shape: (batch_size, timesteps, 1)\n",
        "\n",
        "        # Additional layers\n",
        "        self.repeat_vector = tf.keras.layers.RepeatVector(timesteps, name='repeat_vector')  # Output shape: (batch_size, timesteps, 32)\n",
        "        self.dropout = tf.keras.layers.Dropout(0.2, name='dropout')  # Output shape: (batch_size, timesteps, 16)\n",
        "        self.layer_norm = tf.keras.layers.LayerNormalization(name='layer_normalization')  # Output shape: (batch_size, timesteps, 16)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        # First LSTM processing\n",
        "        x = self.lstm(inputs)  # Output shape: (batch_size, timesteps, 32)\n",
        "\n",
        "        # Process first LSTM output\n",
        "        x = self.dense(x)  # Output shape: (batch_size, timesteps, 4)\n",
        "        x = self.dense_1(x)  # Output shape: (batch_size, timesteps, 4)\n",
        "        x = self.dense_2(x)  # Output shape: (batch_size, timesteps, 32)\n",
        "\n",
        "        # Second LSTM processing\n",
        "        x = self.lstm_1(x)  # Output shape: (batch_size, timesteps, 32)\n",
        "\n",
        "        # Get last timestep for repeat vector\n",
        "        last_timestep = x[:, -1, :]  # Output shape: (batch_size, 32)\n",
        "        x = self.repeat_vector(last_timestep)  # Output shape: (batch_size, timesteps, 32)\n",
        "\n",
        "        # Process repeated vector\n",
        "        x = self.dense_3(x)  # Output shape: (batch_size, timesteps, 32)\n",
        "        x = self.dense_4(x)  # Output shape: (batch_size, timesteps, 16)\n",
        "\n",
        "        # Apply dropout during training\n",
        "        if training:\n",
        "            x = self.dropout(x)  # Output shape: (batch_size, timesteps, 16)\n",
        "\n",
        "        # Apply layer normalization\n",
        "        x = self.layer_norm(x)  # Output shape: (batch_size, timesteps, 16)\n",
        "\n",
        "        # Generate outputs through two branches\n",
        "        output1 = self.dense_5(x)  # Output shape: (batch_size, timesteps, 1)\n",
        "        x = self.dense_6(x)  # Output shape: (batch_size, timesteps, 16)\n",
        "        output2 = self.dense_7(x)  # Output shape: (batch_size, timesteps, 1)\n",
        "\n",
        "        # Combine outputs\n",
        "        outputs = tf.concat([output1, output2], axis=-1)  # Output shape: (batch_size, timesteps, 2)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(LSTM_VAE, self).get_config()\n",
        "        config.update({\n",
        "            'timesteps': self.timesteps,\n",
        "            'n_features': self.n_features,\n",
        "            'latent_dim': self.latent_dim\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "def reshape(da):\n",
        "    \"\"\"Reshape dados para formato LSTM\"\"\"\n",
        "    data = []\n",
        "    for i in range(len(da) - timesteps + 1):\n",
        "        data.append(da[i:(i + timesteps)])\n",
        "    return np.array(data)\n",
        "\n",
        "def prepare_training_data(data, batch_size=128):\n",
        "    \"\"\"Prepara dados para o modelo\"\"\"\n",
        "    data = reshape(data)\n",
        "    data = data.reshape(-1, timesteps, n_features)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
        "    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "def calculate_advanced_metrics(predictions, originals):\n",
        "    \"\"\"Calculate advanced evaluation metrics\"\"\"\n",
        "    mse = np.mean(np.square(predictions - originals))\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = np.mean(np.abs(predictions - originals))\n",
        "\n",
        "    # Evitar divisão por zero\n",
        "    mape = np.mean(np.abs((originals - predictions) / (originals + 1e-6))) * 100\n",
        "\n",
        "    # Métricas específicas para valores extremos\n",
        "    percentiles = [1, 5, 25, 50, 75, 95, 99]\n",
        "    orig_percentiles = np.percentile(originals, percentiles)\n",
        "    pred_percentiles = np.percentile(predictions, percentiles)\n",
        "\n",
        "    percentile_errors = {\n",
        "        f'p{p}_error': abs(o - p)\n",
        "        for p, o, p in zip(percentiles, orig_percentiles, pred_percentiles)\n",
        "    }\n",
        "\n",
        "    # Log likelihood aproximado (assumindo distribuição normal)\n",
        "    residuals = predictions - originals\n",
        "    std = np.std(residuals) + 1e-6\n",
        "    log_px = -0.5 * np.mean(np.square(residuals) / std + np.log(2 * np.pi * std))\n",
        "\n",
        "    metrics = {\n",
        "        'mse': mse,\n",
        "        'rmse': rmse,\n",
        "        'mae': mae,\n",
        "        'mape': mape,\n",
        "        'log_px': log_px,\n",
        "        **percentile_errors\n",
        "    }\n",
        "\n",
        "    return metrics, log_px\n",
        "\n",
        "def save_monthly_results(predictions, originals, log_px, df_original, file_name, mes):\n",
        "    \"\"\"Save monthly analysis results to Excel file with multiple sheets\"\"\"\n",
        "    try:\n",
        "        # Create output filename\n",
        "        base_name = os.path.splitext(file_name)[0]\n",
        "        output_file = os.path.join(results_dir, f'analise_mensal_{mes}_{base_name}.xlsx')\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics, _ = calculate_advanced_metrics(predictions, originals)\n",
        "\n",
        "        # Calculate monthly statistics\n",
        "        monthly_stats = calcular_estatisticas_mensais(df_original, predictions, originals)\n",
        "\n",
        "        # Create DataFrames for each sheet\n",
        "        summary_df = pd.DataFrame({\n",
        "            'Metric': metrics.keys(),\n",
        "            'Value': metrics.values()\n",
        "        })\n",
        "\n",
        "        detailed_df = pd.DataFrame({\n",
        "            'Data': df_original.index,\n",
        "            'Original': originals,\n",
        "            'Predicted': predictions,\n",
        "            'Log_Likelihood': log_px,\n",
        "            'Error': originals - predictions,\n",
        "            'Error_Abs': np.abs(originals - predictions),\n",
        "            'Error_Pct': np.abs((originals - predictions) / (originals + 1e-6)) * 100,\n",
        "            'Year': df_original.index.year,\n",
        "            'Month': df_original.index.month\n",
        "        })\n",
        "\n",
        "        # Análise anual\n",
        "        annual_stats = detailed_df.groupby('Year').agg({\n",
        "            'Original': ['mean', 'std', 'min', 'max', lambda x: np.percentile(x, 5), lambda x: np.percentile(x, 95)],\n",
        "            'Predicted': ['mean', 'std', 'min', 'max', lambda x: np.percentile(x, 5), lambda x: np.percentile(x, 95)],\n",
        "            'Error': ['mean', 'std'],\n",
        "            'Error_Abs': ['mean', 'max'],\n",
        "            'Error_Pct': ['mean', 'max'],\n",
        "            'Log_Likelihood': ['mean', 'std', 'min']\n",
        "        })\n",
        "\n",
        "        # Rename columns for better readability\n",
        "        annual_stats.columns = [\n",
        "            'Original_Mean', 'Original_Std', 'Original_Min', 'Original_Max', 'Original_P5', 'Original_P95',\n",
        "            'Predicted_Mean', 'Predicted_Std', 'Predicted_Min', 'Predicted_Max', 'Predicted_P5', 'Predicted_P95',\n",
        "            'Error_Mean', 'Error_Std',\n",
        "            'AbsError_Mean', 'AbsError_Max',\n",
        "            'PctError_Mean', 'PctError_Max',\n",
        "            'LogLik_Mean', 'LogLik_Std', 'LogLik_Min'\n",
        "        ]\n",
        "\n",
        "        # Save all DataFrames to Excel\n",
        "        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
        "            summary_df.to_excel(writer, sheet_name='Resumo', index=False)\n",
        "            detailed_df.to_excel(writer, sheet_name='Dados Detalhados', index=True)\n",
        "            pd.DataFrame(monthly_stats).transpose().to_excel(writer, sheet_name='Estatísticas Mensais')\n",
        "            annual_stats.to_excel(writer, sheet_name='Análise Anual')\n",
        "\n",
        "        logging.info(f\"Results saved to {output_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error saving results to Excel: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def load_model_and_scaler(month):\n",
        "    \"\"\"Load model and scaler for a specific month\"\"\"\n",
        "    try:\n",
        "        # Create model with custom object scope\n",
        "        with tf.keras.utils.custom_object_scope({\n",
        "            'LSTM_VAE': LSTM_VAE,\n",
        "            'ReparameterizationLayer': ReparameterizationLayer,\n",
        "            'RepeatVectorLayer': RepeatVectorLayer,\n",
        "            'TemperatureWeightLayer': TemperatureWeightLayer\n",
        "        }):\n",
        "            # Create new LSTM_VAE instance\n",
        "            model = LSTM_VAE(timesteps=timesteps, n_features=n_features, latent_dim=latent_dim)\n",
        "\n",
        "            # Build model with input shape\n",
        "            dummy_input = tf.zeros((1, timesteps, n_features))\n",
        "            _ = model(dummy_input, training=False)\n",
        "\n",
        "            # Load weights\n",
        "            model_path = os.path.join(model_dir, f'lstm_vae_model_{month}.h5')\n",
        "            model.load_weights(model_path)\n",
        "\n",
        "            # Load scaler\n",
        "            scaler_path = os.path.join(model_dir, f'scaler_{month}.pkl')\n",
        "            with open(scaler_path, 'rb') as f:\n",
        "                scaler = pickle.load(f)\n",
        "\n",
        "            logging.info(f\"Successfully loaded model and scaler for month {month}\")\n",
        "\n",
        "            return model, scaler\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading model/scaler for month {month}: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def calcular_estatisticas_mensais(df_original, predictions, originals):\n",
        "    \"\"\"Calculate detailed monthly statistics\"\"\"\n",
        "    try:\n",
        "        # Estatísticas básicas\n",
        "        stats = {\n",
        "            'Média Original': np.mean(originals),\n",
        "            'Desvio Padrão Original': np.std(originals),\n",
        "            'Mínimo Original': np.min(originals),\n",
        "            'Máximo Original': np.max(originals),\n",
        "            'Média Prevista': np.mean(predictions),\n",
        "            'Desvio Padrão Previsto': np.std(predictions),\n",
        "            'Mínimo Previsto': np.min(predictions),\n",
        "            'Máximo Previsto': np.max(predictions)\n",
        "        }\n",
        "\n",
        "        # Análise de valores extremos\n",
        "        percentiles = [1, 5, 25, 50, 75, 95, 99]\n",
        "        for p in percentiles:\n",
        "            stats[f'Percentil {p}% Original'] = np.percentile(originals, p)\n",
        "            stats[f'Percentil {p}% Previsto'] = np.percentile(predictions, p)\n",
        "\n",
        "        # Análise de erro por faixa de temperatura\n",
        "        mean_temp = np.mean(originals)\n",
        "        std_temp = np.std(originals)\n",
        "\n",
        "        # Definir faixas de temperatura\n",
        "        ranges = [\n",
        "            ('Extremamente Baixo', lambda x: x < mean_temp - 2*std_temp),\n",
        "            ('Muito Baixo', lambda x: (x >= mean_temp - 2*std_temp) & (x < mean_temp - std_temp)),\n",
        "            ('Baixo', lambda x: (x >= mean_temp - std_temp) & (x < mean_temp - 0.5*std_temp)),\n",
        "            ('Normal', lambda x: (x >= mean_temp - 0.5*std_temp) & (x <= mean_temp + 0.5*std_temp)),\n",
        "            ('Alto', lambda x: (x > mean_temp + 0.5*std_temp) & (x <= mean_temp + std_temp)),\n",
        "            ('Muito Alto', lambda x: (x > mean_temp + std_temp) & (x <= mean_temp + 2*std_temp)),\n",
        "            ('Extremamente Alto', lambda x: x > mean_temp + 2*std_temp)\n",
        "        ]\n",
        "\n",
        "        # Calcular métricas por faixa\n",
        "        for range_name, range_func in ranges:\n",
        "            mask = range_func(originals)\n",
        "            if np.any(mask):\n",
        "                range_orig = originals[mask]\n",
        "                range_pred = predictions[mask]\n",
        "\n",
        "                mse = np.mean(np.square(range_pred - range_orig))\n",
        "                mae = np.mean(np.abs(range_pred - range_orig))\n",
        "                mape = np.mean(np.abs((range_orig - range_pred) / (range_orig + 1e-6))) * 100\n",
        "\n",
        "                stats[f'{range_name} - Contagem'] = np.sum(mask)\n",
        "                stats[f'{range_name} - MSE'] = mse\n",
        "                stats[f'{range_name} - MAE'] = mae\n",
        "                stats[f'{range_name} - MAPE'] = mape\n",
        "\n",
        "        return pd.Series(stats)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error calculating monthly statistics: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def process_file(file_path):\n",
        "    \"\"\"Process a single file with monthly analysis\"\"\"\n",
        "    try:\n",
        "        logging.info(f\"Processing file: {file_path}\")\n",
        "        file_name = os.path.basename(file_path)\n",
        "\n",
        "        # Read data\n",
        "        df = pd.read_excel(file_path)\n",
        "\n",
        "        # Convert date column if needed\n",
        "        if 'Data' not in df.columns and 'DATA' in df.columns:\n",
        "            df = df.rename(columns={'DATA': 'Data'})\n",
        "\n",
        "        # Convert date using Brazilian format (dd/mm/yyyy)\n",
        "        df['Data'] = pd.to_datetime(df['Data'], format='%d/%m/%Y')\n",
        "        df.set_index('Data', inplace=True)\n",
        "\n",
        "        # Sort index to ensure chronological order\n",
        "        df = df.sort_index()\n",
        "\n",
        "        # Process each month\n",
        "        for month in range(1, 13):\n",
        "            mes = meses[month]\n",
        "            logging.info(f\"Processing month: {mes}\")\n",
        "\n",
        "            try:\n",
        "                # Load model and scaler for the month\n",
        "                model, scaler = load_model_and_scaler(mes)\n",
        "\n",
        "                # Filter data for the month\n",
        "                df_month = df[df.index.month == month].copy()\n",
        "                if len(df_month) < timesteps:\n",
        "                    logging.warning(f\"Insufficient data for month {mes}. Skipping...\")\n",
        "                    continue\n",
        "\n",
        "                # Prepare data\n",
        "                if 'Temperatura Maxima' in df_month.columns:\n",
        "                    data = df_month['Temperatura Maxima'].values.reshape(-1, 1)\n",
        "                elif 'TEMPERATURA MAXIMA' in df_month.columns:\n",
        "                    data = df_month['TEMPERATURA MAXIMA'].values.reshape(-1, 1)\n",
        "                else:\n",
        "                    raise ValueError(\"Temperature column not found in data\")\n",
        "\n",
        "                # Remove any missing values\n",
        "                if np.any(np.isnan(data)):\n",
        "                    logging.warning(f\"Found {np.sum(np.isnan(data))} missing values in month {mes}\")\n",
        "                    data = data[~np.isnan(data)]\n",
        "\n",
        "                data_scaled = scaler.transform(data)\n",
        "\n",
        "                # Create sequences\n",
        "                sequences = reshape(data_scaled)\n",
        "\n",
        "                # Generate predictions\n",
        "                predictions_scaled = model.predict(sequences)\n",
        "                predictions_scaled = predictions_scaled[:, -1, :]  # Get last timestep\n",
        "\n",
        "                # Inverse transform predictions\n",
        "                predictions = scaler.inverse_transform(predictions_scaled)\n",
        "\n",
        "                # Calculate log likelihood\n",
        "                residuals = predictions - data[-len(predictions):]\n",
        "                std = np.std(residuals) + 1e-6\n",
        "                log_px = -0.5 * np.square(residuals) / std - 0.5 * np.log(2 * np.pi * std)\n",
        "\n",
        "                # Save results\n",
        "                save_monthly_results(\n",
        "                    predictions.flatten(),\n",
        "                    data[-len(predictions):].flatten(),\n",
        "                    log_px.flatten(),\n",
        "                    df_month[-len(predictions):],\n",
        "                    file_name,\n",
        "                    mes\n",
        "                )\n",
        "\n",
        "                logging.info(f\"Successfully processed month {mes}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error processing month {mes}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        logging.info(f\"Finished processing file: {file_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing file {file_path}: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function with error handling and progress tracking\"\"\"\n",
        "    try:\n",
        "        # Get list of Excel files\n",
        "        excel_files = glob.glob(os.path.join(data_dir, \"*.xlsx\"))\n",
        "\n",
        "        if not excel_files:\n",
        "            logging.warning(f\"No Excel files found in directory: {data_dir}\")\n",
        "            return\n",
        "\n",
        "        logging.info(f\"Found {len(excel_files)} Excel files to process\")\n",
        "\n",
        "        # Process each file\n",
        "        for file_path in tqdm(excel_files, desc=\"Processing files\"):\n",
        "            try:\n",
        "                process_file(file_path)\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Failed to process file {file_path}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        logging.info(\"Finished processing all files\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in main function: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "oXd3krxYDTAN",
        "outputId": "35608fc5-988b-4fc3-cf69-ee28d10330d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name '__file__' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-582fb1a0b92c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Diretórios\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mbase_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Convencionais processadas temperaturas\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lstm_vae_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0xye0llDMuo"
      },
      "outputs": [],
      "source": []
    }
  ]
}