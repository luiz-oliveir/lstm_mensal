{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/luiz-oliveir/lstm_mensal/blob/main/LSTM_VAE_Mensal_v2.ipynb",
      "authorship_tag": "ABX9TyO/TWrNTLNCJtLHrWBwvpro",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luiz-oliveir/lstm_mensal/blob/main/LSTM_VAE_Mensal_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/luiz-oliveir/LSTM_mensal.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jrk0Xsz0DZF-",
        "outputId": "86e1b14c-b13c-4d6b-9100-08cfd673367f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LSTM_mensal'...\n",
            "remote: Enumerating objects: 107, done.\u001b[K\n",
            "remote: Counting objects: 100% (107/107), done.\u001b[K\n",
            "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
            "remote: Total 107 (delta 23), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (107/107), 14.21 MiB | 19.61 MiB/s, done.\n",
            "Resolving deltas: 100% (23/23), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "LSTM VAE Model for Monthly Temperature Analysis - Google Colab Version\n",
        "\"\"\"\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import shutil\n",
        "import logging\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# Disable GPU for Colab (if causing memory issues)\n",
        "try:\n",
        "    tf.config.set_visible_devices([], 'GPU')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Diretórios\n",
        "lstm_root = '/content/drive/MyDrive/LSTM'\n",
        "base_dir = os.path.join(lstm_root, 'lstm mensal')\n",
        "data_dir = os.path.join(lstm_root, 'Convencionais processadas temperaturas')\n",
        "model_dir = os.path.join(base_dir, 'lstm_vae_model')\n",
        "images_dir = os.path.join(base_dir, 'lstm_vae_images')\n",
        "results_dir = os.path.join(base_dir, 'Resumo resultados')\n",
        "\n",
        "def copy_data_file():\n",
        "    \"\"\"Copy data file to Google Drive if needed\"\"\"\n",
        "    target_file = os.path.join(data_dir, '82024.xlsx')\n",
        "\n",
        "    # Check if file already exists in Google Drive\n",
        "    if os.path.exists(target_file):\n",
        "        print(f\"\\nData file already exists in Google Drive: {target_file}\")\n",
        "        return True\n",
        "\n",
        "    # Try to find the file in Colab environment\n",
        "    possible_locations = [\n",
        "        '/content/82024.xlsx',  # Root Colab directory\n",
        "        '/content/sample_data/82024.xlsx',  # Sample data directory\n",
        "        '/content/drive/MyDrive/82024.xlsx'  # Google Drive root\n",
        "    ]\n",
        "\n",
        "    source_file = None\n",
        "    for loc in possible_locations:\n",
        "        if os.path.exists(loc):\n",
        "            source_file = loc\n",
        "            break\n",
        "\n",
        "    if source_file:\n",
        "        print(f\"\\nCopying data file from {source_file} to {target_file}\")\n",
        "        shutil.copy2(source_file, target_file)\n",
        "        return True\n",
        "\n",
        "    print(\"\\nError: Could not find 82024.xlsx\")\n",
        "    print(\"Please upload the file using one of these methods:\")\n",
        "    print(\"1. Upload directly to Google Drive in the 'Convencionais processadas temperaturas' folder\")\n",
        "    print(\"2. Upload to Colab using the file browser (left sidebar)\")\n",
        "    print(\"\\nFile required: 82024.xlsx\")\n",
        "    return False\n",
        "\n",
        "# Setup Google Drive directories\n",
        "def setup_drive_directories():\n",
        "    \"\"\"Create necessary directories in Google Drive\"\"\"\n",
        "    print(\"\\nSetting up Google Drive directories...\")\n",
        "\n",
        "    # Create all required directories\n",
        "    for dir_path in [lstm_root, base_dir, data_dir, model_dir, images_dir, results_dir]:\n",
        "        if not os.path.exists(dir_path):\n",
        "            os.makedirs(dir_path)\n",
        "            print(f\"Created directory: {dir_path}\")\n",
        "        else:\n",
        "            print(f\"Directory exists: {dir_path}\")\n",
        "\n",
        "# Debug: Print directory existence\n",
        "print(\"\\nVerifying directories:\")\n",
        "print(f\"LSTM root exists: {os.path.exists(lstm_root)}\")\n",
        "print(f\"Base dir exists: {os.path.exists(base_dir)}\")\n",
        "print(f\"Data dir exists: {os.path.exists(data_dir)}\")\n",
        "print(f\"Model dir exists: {os.path.exists(model_dir)}\")\n",
        "print(f\"Images dir exists: {os.path.exists(images_dir)}\")\n",
        "print(f\"Results dir exists: {os.path.exists(results_dir)}\")\n",
        "\n",
        "# Create directories if needed\n",
        "setup_drive_directories()\n",
        "\n",
        "# Copy data file if needed\n",
        "if not copy_data_file():\n",
        "    sys.exit(1)\n",
        "\n",
        "# Validate data directory and files\n",
        "data_files = glob.glob(os.path.join(data_dir, \"*.xlsx\"))\n",
        "if not data_files:\n",
        "    print(f\"\\nError: No Excel files found in the data directory: {data_dir}\")\n",
        "    print(\"\\nPlease ensure 82024.xlsx is in the correct location:\")\n",
        "    print(f\"{os.path.join(data_dir, '82024.xlsx')}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(f\"\\nFound {len(data_files)} Excel file(s):\")\n",
        "for file in data_files:\n",
        "    print(f\"- {os.path.basename(file)}\")\n",
        "\n",
        "# Configurações\n",
        "row_mark = 740\n",
        "batch_size = 128\n",
        "timesteps = 7  # Janela de tempo para análise\n",
        "n_features = 1  # Número de features (temperatura)\n",
        "latent_dim = 32  # Dimensão do espaço latente\n",
        "epoch_num = 100\n",
        "threshold = None\n",
        "\n",
        "# Dicionário de meses\n",
        "meses = {\n",
        "    1:'jan', 2:'fev', 3:'mar', 4:'abr', 5:'mai', 6:'jun',\n",
        "    7:'jul', 8:'ago', 9:'set', 10:'out', 11:'nov', 12:'dez'\n",
        "}\n",
        "\n",
        "class ReparameterizationLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(ReparameterizationLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch_size = tf.shape(z_mean)[0]\n",
        "        latent_dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.random.normal(shape=(batch_size, latent_dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "class RepeatVectorLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, timesteps, **kwargs):\n",
        "        super(RepeatVectorLayer, self).__init__(**kwargs)\n",
        "        self.timesteps = timesteps\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.repeat(tf.expand_dims(inputs, axis=1), repeats=self.timesteps, axis=1)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(RepeatVectorLayer, self).get_config()\n",
        "        config.update({'timesteps': self.timesteps})\n",
        "        return config\n",
        "\n",
        "class TemperatureWeightLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(TemperatureWeightLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, x):\n",
        "        mean = tf.keras.backend.mean(x, axis=[1, 2], keepdims=True)\n",
        "        std = tf.keras.backend.std(x, axis=[1, 2], keepdims=True) + tf.keras.backend.epsilon()\n",
        "        z_scores = tf.abs((x - mean) / std)\n",
        "        weights = tf.exp(z_scores)\n",
        "        weights = weights / (tf.keras.backend.mean(weights, axis=[1, 2], keepdims=True) + tf.keras.backend.epsilon())\n",
        "        return weights\n",
        "\n",
        "class LSTM_VAE(tf.keras.Model):\n",
        "    def __init__(self, timesteps=7, n_features=1, latent_dim=32, **kwargs):\n",
        "        super(LSTM_VAE, self).__init__(**kwargs)\n",
        "        self.timesteps = timesteps\n",
        "        self.n_features = n_features\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # First LSTM layer\n",
        "        self.lstm = tf.keras.layers.LSTM(32, return_sequences=True, name='lstm')\n",
        "\n",
        "        # First LSTM output processing\n",
        "        self.dense = tf.keras.layers.Dense(4, name='dense')\n",
        "        self.dense_1 = tf.keras.layers.Dense(4, name='dense_1')\n",
        "        self.dense_2 = tf.keras.layers.Dense(32, name='dense_2')\n",
        "\n",
        "        # Second LSTM layer\n",
        "        self.lstm_1 = tf.keras.layers.LSTM(32, return_sequences=True, name='lstm_1')\n",
        "\n",
        "        # Second LSTM output processing\n",
        "        self.dense_3 = tf.keras.layers.Dense(32, name='dense_3')\n",
        "        self.dense_4 = tf.keras.layers.Dense(16, name='dense_4')\n",
        "        self.dense_5 = tf.keras.layers.Dense(1, name='dense_5')\n",
        "\n",
        "        # Final processing branch\n",
        "        self.dense_6 = tf.keras.layers.Dense(16, name='dense_6')\n",
        "        self.dense_7 = tf.keras.layers.Dense(1, name='dense_7')\n",
        "\n",
        "        # Additional layers\n",
        "        self.repeat_vector = tf.keras.layers.RepeatVector(timesteps, name='repeat_vector')\n",
        "        self.dropout = tf.keras.layers.Dropout(0.2, name='dropout')\n",
        "        self.layer_norm = tf.keras.layers.LayerNormalization(name='layer_normalization')\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        # First LSTM processing\n",
        "        x = self.lstm(inputs)\n",
        "\n",
        "        # Process first LSTM output\n",
        "        x = self.dense(x)\n",
        "        x = self.dense_1(x)\n",
        "        x = self.dense_2(x)\n",
        "\n",
        "        # Second LSTM processing\n",
        "        x = self.lstm_1(x)\n",
        "\n",
        "        # Get last timestep for repeat vector\n",
        "        last_timestep = x[:, -1, :]\n",
        "        x = self.repeat_vector(last_timestep)\n",
        "\n",
        "        # Process repeated vector\n",
        "        x = self.dense_3(x)\n",
        "        x = self.dense_4(x)\n",
        "\n",
        "        # Apply dropout during training\n",
        "        if training:\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        # Apply layer normalization\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        # Generate outputs through two branches\n",
        "        output1 = self.dense_5(x)\n",
        "        x = self.dense_6(x)\n",
        "        output2 = self.dense_7(x)\n",
        "\n",
        "        # Combine outputs\n",
        "        outputs = tf.concat([output1, output2], axis=-1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(LSTM_VAE, self).get_config()\n",
        "        config.update({\n",
        "            'timesteps': self.timesteps,\n",
        "            'n_features': self.n_features,\n",
        "            'latent_dim': self.latent_dim\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "def reshape(da):\n",
        "    \"\"\"Reshape dados para formato LSTM\"\"\"\n",
        "    data = []\n",
        "    for i in range(len(da) - timesteps + 1):\n",
        "        data.append(da[i:(i + timesteps)])\n",
        "    return np.array(data)\n",
        "\n",
        "def prepare_training_data(data, batch_size=128):\n",
        "    \"\"\"Prepara dados para o modelo\"\"\"\n",
        "    data = reshape(data)\n",
        "    data = data.reshape(-1, timesteps, n_features)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
        "    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "def calculate_advanced_metrics(predictions, originals):\n",
        "    \"\"\"Calculate advanced evaluation metrics\"\"\"\n",
        "    mse = np.mean(np.square(predictions - originals))\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = np.mean(np.abs(predictions - originals))\n",
        "    mape = np.mean(np.abs((originals - predictions) / originals)) * 100\n",
        "\n",
        "    # Calculate log-likelihood metrics\n",
        "    residuals = predictions - originals\n",
        "    std = np.std(residuals)\n",
        "    log_likelihood = -0.5 * np.sum(np.square(residuals) / (2 * np.square(std)) + np.log(2 * np.pi * np.square(std)))\n",
        "\n",
        "    # Calculate percentile metrics\n",
        "    percentiles = [1, 5, 25, 50, 75, 95, 99]\n",
        "    orig_percentiles = np.percentile(originals, percentiles)\n",
        "    pred_percentiles = np.percentile(predictions, percentiles)\n",
        "\n",
        "    percentile_errors = {\n",
        "        f'p{p}_error': abs(o - p)\n",
        "        for p, o, p in zip(percentiles, orig_percentiles, pred_percentiles)\n",
        "    }\n",
        "\n",
        "    metrics = {\n",
        "        'mse': mse,\n",
        "        'rmse': rmse,\n",
        "        'mae': mae,\n",
        "        'mape': mape,\n",
        "        'log_likelihood': log_likelihood,\n",
        "        **percentile_errors\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def train_monthly_models(data_files):\n",
        "    \"\"\"Train separate LSTM VAE models for each month\"\"\"\n",
        "    print(\"\\nStarting monthly model training...\")\n",
        "    print(f\"Model directory: {model_dir}\")\n",
        "    print(f\"Processing file: {os.path.basename(data_files[0])}\")\n",
        "\n",
        "    try:\n",
        "        # Read data\n",
        "        file_path = data_files[0]  # We know we only have one file\n",
        "        df = pd.read_excel(file_path)\n",
        "        df['Data'] = pd.to_datetime(df['Data'])\n",
        "        df = df.set_index('Data')\n",
        "\n",
        "        # Sort index to ensure chronological order\n",
        "        df = df.sort_index()\n",
        "        print(f\"Data range: {df.index.min().strftime('%Y-%m-%d')} to {df.index.max().strftime('%Y-%m-%d')}\")\n",
        "\n",
        "        # Process each month\n",
        "        for month in range(1, 13):\n",
        "            month_name = meses[month]\n",
        "            print(f\"\\nProcessing month: {month_name}\")\n",
        "\n",
        "            # Filter data for current month\n",
        "            month_data = df[df.index.month == month]['Temperatura'].values\n",
        "            print(f\"Number of samples for {month_name}: {len(month_data)}\")\n",
        "\n",
        "            if len(month_data) < batch_size:\n",
        "                print(f\"Warning: Insufficient data for month {month_name}. Minimum required: {batch_size}\")\n",
        "                continue\n",
        "\n",
        "            # Scale data\n",
        "            scaler = MinMaxScaler()\n",
        "            month_data_scaled = scaler.fit_transform(month_data.reshape(-1, 1))\n",
        "\n",
        "            # Save scaler\n",
        "            scaler_path = os.path.join(model_dir, f'scaler_{month_name}.pkl')\n",
        "            with open(scaler_path, 'wb') as f:\n",
        "                pickle.dump(scaler, f)\n",
        "            print(f\"Saved scaler to: {os.path.basename(scaler_path)}\")\n",
        "\n",
        "            # Prepare training data\n",
        "            train_dataset = prepare_training_data(month_data_scaled, batch_size)\n",
        "\n",
        "            # Create and compile model\n",
        "            model = LSTM_VAE(timesteps=timesteps, n_features=n_features, latent_dim=latent_dim)\n",
        "\n",
        "            # Build model with input shape\n",
        "            dummy_input = tf.zeros((1, timesteps, n_features))\n",
        "            _ = model(dummy_input, training=False)\n",
        "\n",
        "            # Compile model\n",
        "            model.compile(\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                loss='mse',\n",
        "                metrics=['mae']\n",
        "            )\n",
        "\n",
        "            # Callbacks\n",
        "            model_path = os.path.join(model_dir, f'lstm_vae_model_{month_name}.h5')\n",
        "            checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "                model_path,\n",
        "                monitor='val_loss',\n",
        "                save_best_only=True,\n",
        "                mode='min',\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=10,\n",
        "                restore_best_weights=True\n",
        "            )\n",
        "\n",
        "            reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.2,\n",
        "                patience=5,\n",
        "                min_lr=0.0001\n",
        "            )\n",
        "\n",
        "            # Train model\n",
        "            print(f\"Training model for {month_name}...\")\n",
        "            history = model.fit(\n",
        "                train_dataset,\n",
        "                epochs=epoch_num,\n",
        "                validation_split=0.2,\n",
        "                callbacks=[checkpoint, early_stopping, reduce_lr],\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            # Plot training history\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(history.history['loss'], label='Training Loss')\n",
        "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "            plt.title(f'Model Loss - {month_name}')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.legend()\n",
        "            history_plot_path = os.path.join(images_dir, f'training_history_{month_name}.png')\n",
        "            plt.savefig(history_plot_path)\n",
        "            plt.close()\n",
        "\n",
        "            print(f\"Completed training for {month_name}\")\n",
        "            print(f\"Model saved as: lstm_vae_model_{month_name}.h5\")\n",
        "            print(f\"Training plot saved as: training_history_{month_name}.png\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in training process: {str(e)}\")\n",
        "        print(f\"Error during training: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def analyze_monthly_data(data_files):\n",
        "    \"\"\"Analyze data using monthly models\"\"\"\n",
        "    print(\"\\nStarting monthly data analysis...\")\n",
        "    print(f\"Results directory: {results_dir}\")\n",
        "    print(f\"Processing file: {os.path.basename(data_files[0])}\")\n",
        "\n",
        "    try:\n",
        "        # Read data\n",
        "        file_path = data_files[0]  # We know we only have one file\n",
        "        filename = os.path.basename(file_path)\n",
        "        df = pd.read_excel(file_path)\n",
        "        df['Data'] = pd.to_datetime(df['Data'])\n",
        "        df = df.set_index('Data')\n",
        "        print(f\"Analyzing data from {df.index.min().strftime('%Y-%m-%d')} to {df.index.max().strftime('%Y-%m-%d')}\")\n",
        "\n",
        "        # Process each month\n",
        "        for month in range(1, 13):\n",
        "            month_name = meses[month]\n",
        "            print(f\"\\nAnalyzing month: {month_name}\")\n",
        "\n",
        "            # Filter data for current month\n",
        "            month_data = df[df.index.month == month]['Temperatura'].values\n",
        "            print(f\"Number of samples: {len(month_data)}\")\n",
        "\n",
        "            if len(month_data) < batch_size:\n",
        "                print(f\"Warning: Insufficient data for month {month_name}. Skipping...\")\n",
        "                continue\n",
        "\n",
        "            # Load scaler and model\n",
        "            scaler_path = os.path.join(model_dir, f'scaler_{month_name}.pkl')\n",
        "            model_path = os.path.join(model_dir, f'lstm_vae_model_{month_name}.h5')\n",
        "\n",
        "            if not os.path.exists(scaler_path) or not os.path.exists(model_path):\n",
        "                print(f\"Warning: Model or scaler not found for {month_name}. Skipping...\")\n",
        "                continue\n",
        "\n",
        "            print(f\"Loading model and scaler for {month_name}\")\n",
        "            with open(scaler_path, 'rb') as f:\n",
        "                scaler = pickle.load(f)\n",
        "\n",
        "            model = tf.keras.models.load_model(\n",
        "                model_path,\n",
        "                custom_objects={\n",
        "                    'LSTM_VAE': LSTM_VAE,\n",
        "                    'ReparameterizationLayer': ReparameterizationLayer,\n",
        "                    'RepeatVectorLayer': RepeatVectorLayer,\n",
        "                    'TemperatureWeightLayer': TemperatureWeightLayer\n",
        "                }\n",
        "            )\n",
        "\n",
        "            # Scale data and make predictions\n",
        "            month_data_scaled = scaler.transform(month_data.reshape(-1, 1))\n",
        "            test_dataset = prepare_training_data(month_data_scaled, batch_size)\n",
        "            predictions_scaled = model.predict(test_dataset)\n",
        "            predictions = scaler.inverse_transform(predictions_scaled[:, :, 0])\n",
        "\n",
        "            # Calculate metrics\n",
        "            metrics = calculate_advanced_metrics(predictions.flatten(), month_data)\n",
        "\n",
        "            # Create yearly analysis\n",
        "            yearly_metrics = {}\n",
        "            years = df[df.index.month == month].index.year.unique()\n",
        "            print(f\"Analyzing years: {', '.join(map(str, years))}\")\n",
        "\n",
        "            for year in years:\n",
        "                year_data = df[(df.index.month == month) & (df.index.year == year)]['Temperatura'].values\n",
        "                year_predictions = predictions[df[df.index.month == month].index.year == year].flatten()\n",
        "                if len(year_data) > 0:\n",
        "                    yearly_metrics[year] = calculate_advanced_metrics(year_predictions, year_data)\n",
        "\n",
        "            # Save results\n",
        "            results_filename = f'analise_mensal_{month_name}_{filename}'\n",
        "            results_path = os.path.join(results_dir, results_filename)\n",
        "\n",
        "            writer = pd.ExcelWriter(results_path, engine='openpyxl')\n",
        "\n",
        "            # Save all sheets\n",
        "            pd.DataFrame([metrics]).to_excel(writer, sheet_name='Summary')\n",
        "            pd.DataFrame({\n",
        "                'Original': month_data,\n",
        "                'Predicted': predictions.flatten(),\n",
        "                'Error': predictions.flatten() - month_data\n",
        "            }).to_excel(writer, sheet_name='Detailed Data')\n",
        "\n",
        "            pd.DataFrame({\n",
        "                'Metric': list(metrics.keys()),\n",
        "                'Value': list(metrics.values())\n",
        "            }).to_excel(writer, sheet_name='Statistical Analysis')\n",
        "\n",
        "            pd.DataFrame(yearly_metrics).T.to_excel(writer, sheet_name='Yearly Analysis')\n",
        "\n",
        "            writer.close()\n",
        "            print(f\"Results saved as: {results_filename}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in analysis process: {str(e)}\")\n",
        "        print(f\"Error during analysis: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Get list of data files\n",
        "    data_files = glob.glob(os.path.join(data_dir, \"*.xlsx\"))\n",
        "\n",
        "    if not data_files:\n",
        "        print(f\"\\nError: No Excel files found in the data directory: {data_dir}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    if len(data_files) > 1:\n",
        "        print(f\"\\nWarning: Multiple Excel files found in {data_dir}\")\n",
        "        print(\"Using only the first file:\", os.path.basename(data_files[0]))\n",
        "\n",
        "    print(\"\\nInitializing LSTM VAE Monthly Analysis\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Data file: {os.path.basename(data_files[0])}\")\n",
        "    print(f\"Model directory: {model_dir}\")\n",
        "    print(f\"Results directory: {results_dir}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    try:\n",
        "        # Train models\n",
        "        print(\"\\nPhase 1: Model Training\")\n",
        "        print(\"=\"*50)\n",
        "        train_monthly_models(data_files)\n",
        "        print(\"\\nModel training completed successfully!\")\n",
        "\n",
        "        # Analyze data\n",
        "        print(\"\\nPhase 2: Data Analysis\")\n",
        "        print(\"=\"*50)\n",
        "        analyze_monthly_data(data_files)\n",
        "        print(\"\\nData analysis completed successfully!\")\n",
        "\n",
        "        print(\"\\nProcess completed!\")\n",
        "        print(\"=\"*50)\n",
        "        print(\"Outputs can be found in:\")\n",
        "        print(f\"1. Models: {model_dir}\")\n",
        "        print(f\"2. Training plots: {images_dir}\")\n",
        "        print(f\"3. Analysis results: {results_dir}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\nError: Process failed!\")\n",
        "        print(str(e))\n",
        "        sys.exit(1)\n"
      ],
      "metadata": {
        "id": "oXd3krxYDTAN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "outputId": "51ff0526-ad79-47dc-c2f0-712e15acb8dd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "Verifying directories:\n",
            "LSTM root exists: True\n",
            "Base dir exists: True\n",
            "Data dir exists: True\n",
            "Model dir exists: True\n",
            "Images dir exists: True\n",
            "Results dir exists: True\n",
            "\n",
            "Setting up Google Drive directories...\n",
            "Directory exists: /content/drive/MyDrive/LSTM\n",
            "Directory exists: /content/drive/MyDrive/LSTM/lstm mensal\n",
            "Directory exists: /content/drive/MyDrive/LSTM/Convencionais processadas temperaturas\n",
            "Directory exists: /content/drive/MyDrive/LSTM/lstm mensal/lstm_vae_model\n",
            "Directory exists: /content/drive/MyDrive/LSTM/lstm mensal/lstm_vae_images\n",
            "Directory exists: /content/drive/MyDrive/LSTM/lstm mensal/Resumo resultados\n",
            "\n",
            "Error: Could not find 82024.xlsx\n",
            "Please upload the file using one of these methods:\n",
            "1. Upload directly to Google Drive in the 'Convencionais processadas temperaturas' folder\n",
            "2. Upload to Colab using the file browser (left sidebar)\n",
            "\n",
            "File required: 82024.xlsx\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "1",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0xye0llDMuo"
      },
      "outputs": [],
      "source": []
    }
  ]
}